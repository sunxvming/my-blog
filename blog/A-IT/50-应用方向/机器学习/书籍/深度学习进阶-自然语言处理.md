
本书则侧重于循环神经网络和自然语言处理。本书详细介绍了单词向量、LSTM、seq2seq和Attention等自然语言处理中重要的深度学习技术。
简言之，自然语言处理是让计算机理解我们日常所说的语言的技术。

深度学习极大地改善了传统自然语言处理的性能。比如，谷歌的机器翻译性能就基于深度学习获得了显著提升



## 第1章　神经网络的复习
### 1.1　数学和Python的复习
- 向量和矩阵
将向量和矩阵扩展到N维的数据集合，就是张量。
- 矩阵的对应元素的运算
- 广播
- 向量内积和矩阵乘积
- 矩阵的形状检查
### 1.2　神经网络的推理
- 神经网络的推理的全貌图
- 层的类化及正向传播的实现
### 1.3　神经网络的学习
- 损失函数
- 导数和梯度
- 链式法则
- 计算图
- 梯度的推导和反向传播的实现
- 权重的更新
### 1.4　使用神经网络解决问题
- 螺旋状数据集
- 神经网络的实现
- 学习用的代码
- Trainer 类
### 1.5　计算的高速化
- 位精度
	- NumPy 的浮点数默认使用 64 位的数据类型
	- 使用 32 位浮点数也可以无损地（识别精度几乎不下降）进行神经网络的推理和学习
	- 如果只是神经网络的推理，则即使使用 16 位浮点数进行计算，精度也基本上不会下降，不过，虽然 NumPy 中准备有16 位浮点数，但是普通 CPU 或 GPU 中的运算是用 32 位执行的。处理速度方面并不能获得什么好处
	- 但是，如果是要（在外部文件中）保存学习好的权重，则 16 位浮点数是有用的。具体地说，将权重数据用 16 位精度保存时，只需要 32 位时的一半容量
- GPU（CuPy）
	- CuPy 是基于 GPU 进行并行计算的库。要使用 CuPy，需要使用安装有 NVIDIA 的 GPU 的机器，并且需要安装 CUDA 这个面向 GPU 的通用并行计算平台。
	- CuPy 和 NumPy 拥有共同的 API

## 第2章　自然语言和单词的分布式表示
### 2.1　什么是自然语言处理
我们平常使用的语言，如日语或英语，称为自然语言（natural language）。所谓**自然语言处理**（Natural Language Processing，NLP），顾名思义，就是处理自然语言的科学
简单地说，它是一种能够让计算机理解人类语言的技术。换言之，自然语言处理的目标就是让计算机理解人说的话，进而完成对我们有帮助的事情。
问答系统是自然语言处理技术的一个应用
### 2.2　同义词词典
![image.png](https://sxm-upload.oss-cn-beijing.aliyuncs.com/imgs/20231218145734.png)

像这样，通过对所有单词创建近义词集合，并用图表示各个单词的关系，可以定义单词之间的联系。利用这个“单词网络”，可以教会计算机单词之间的相关性



WordNet
同义词词典的问题
### 2.3　基于计数的方法
基于Python的语料库的预处理
单词的分布式表示
分布式假设
共现矩阵
向量间的相似度
相似单词的排序
### 2.4　基于计数的方法的改进
点互信息
降维
基于SVD的降维
PTB 数据集
基于PTB 数据集的评价
### 2.5　小结

## 第3章　word2vec
### 3.1　基于推理的方法和神经网络
基于计数的方法的问题
基于推理的方法的概要
神经网络中单词的处理方法
### 3.2　简单的word2vec
CBOW模型的推理
CBOW模型的学习
word2vec的权重和分布式表示
### 3.3　学习数据的准备
上下文和目标词
转化为one-hot表示
### 3.4　CBOW 模型的实现
### 3.5　word2vec的补充说明
CBOW模型和概率
skip-gram模型
基于计数与基于推理
### 3.6　小结

## 第4章　word2vec的高速化
### 4.1　word2vec的改进①
Embedding层
Embedding层的实现
### 4.2　word2vec的改进②
中间层之后的计算问题
从多分类到二分类
sigmoid 函数和交叉熵误差
多分类到二分类的实现
负采样
负采样的采样方法
负采样的实现
### 4.3　改进版word2vec的学习
CBOW模型的实现
CBOW模型的学习代码
CBOW模型的评价
### 4.4　wor2vec相关的其他话题
word2vec的应用例
单词向量的评价方法
### 4.5　小结

## 第5章　RNN
### 5.1　概率和语言模型
概率视角下的word2vec
语言模型
将CBOW模型用作语言模型？
### 5.2　RNN
循环的神经网络
展开循环
Backpropagation Through Time
Truncated BPTT
Truncated BPTT的mini-batch学习
### 5.3　RNN的实现
RNN 层的实现
Time RNN层的实现
### 5.4　处理时序数据的层的实现本
RNNLM的全貌图
Time 层的实现
### 5.5　RNNLM的学习和评价
RNNLM的实现
语言模型的评价
RNNLM的学习代码
RNNLM的Trainer 类
### 5.6　小结

## 第6章　Gated RNN
### 6.1　RNN的问题
RNN 的复习
梯度消失和梯度爆炸
梯度消失和梯度爆炸的原因
梯度爆炸的对策
### 6.2　梯度消失和LSTM
LSTM的接口
LSTM层的结构
输出门
遗忘门
新的记忆单元
输入门
LSTM的梯度的流动
### 6.3　LSTM 的实现
### 6.4　使用LSTM 的语言模型
### 6.5　进一步改进RNNLM
LSTM层的多层化
基于Dropout抑制过拟合
权重共享
更好的RNNLM的实现
前沿研究
### 6.6　小结
## 第7章　基于RNN 生成文本

### 7.1　使用语言模型生成文本
使用RNN 生成文本的步骤
文本生成的实现
更好的文本生成
### 7.2　seq2seq 模型
seq2seq的原理
时序数据转换的简单尝试
可变长度的时序数据
加法数据集
### 7.3　seq2seq 的实现
Encoder类
Decoder类
Seq2seq类
seq2seq的评价
### 7.4　seq2seq 的改进
反转输入数据（Reverse）
偷窥（Peeky）
### 7.5　seq2seq的应用
聊天机器人
算法学习
自动图像描述
### 7.6　小结

## 第8章　Attention
### 8.1　Attention的结构
seq2seq存在的问题
编码器的改进
解码器的改进①
解码器的改进②
解码器的改进③
### 8.2　带Attention的seq2seq的实现
编码器的实现
解码器的实现
seq2seq的实现
### 8.3　Attention的评价
日期格式转换问题
带Attention的seq2seq的学习
Attention的可视化
### 8.4　关于Attention的其他话题
双向RNN
Attention层的使用方法
seq2seq的深层化和skip connection
### 8.5　Attention的应用
GNMT
Transformer
NTM
### 8.6　小结
