市面上用于做训练和做推理的显卡都很多，比如英伟达、华为、寒武纪、曙光等公司都有自己的训练卡和推理卡。拿英伟达来说，训练时可以选A100\V100\A800等显卡，推理时可选P4、T4、A10等显卡。但网上很少有资料能清楚地说明训练卡和推理卡的主要区别。他们设计时主要考虑的因素有哪些？训练卡用于做推理会有什么问题吗？


首先要了解神经网络训练和推理时的差别，然后自然就知道对GPU的需求的差别。  
先看一下训练时的需求。神经网络训练通常使用随机梯度下降算法，显存中除了加载模型参数，还需要保存中间状态，主要是梯度信息，相比推理，显存需求要增加几倍，显存要够大才能跑起来；要训练好的模型，需要使用大量数据，大量数据要读入显存，显存带宽要够大；另外对于当前的大数据量，单卡已经无法满足要求，要用多卡集群训练，集群训练要在多机间通信，要交换大量数据，要支持更高的通信带宽，接口一般用NVLINK，通常还要GPU支持RDMA特性，能够直接在显存和通信卡内存间搬数据。

总结起来就是训练卡要求显存大，显存带宽大，和外部通信接口带宽大，算力就不说了，都不是主要考虑问题了，训练卡目前主要是NVIDIA的A100 V100。

推理时的需求就简单了，算力和显存平衡就可了，模型能装的进去，把算力跑慢就可以了，显存和算力越大，推理的并发数越多，T4跑推理挺好，便宜，算力也够强。

