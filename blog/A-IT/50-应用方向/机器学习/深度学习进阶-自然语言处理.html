<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Style-Type" content="text/css" />
<meta name="generator" content="pandoc" />




<link rel="stylesheet" href="../../../.././css/style.css" />




<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Saira+Semi+Condensed%3A400%2C700&ver=4.9.18" type="text/css" />

<script src="https://libs.baidu.com/jquery/2.0.0/jquery.min.js"></script>
<script>
jQuery(document).ready(function(){
    jQuery('pre').each(function(){
        var el = jQuery(this).find('code');
        var code_block = el.html(); 
 
        if (el.length > 0) { 
            jQuery(this).addClass('prettyprint').html(code_block).attr('style', 'max-height:450px');
        } else { 
            jQuery(this).removeClass().addClass('prettyprint'); 
        }
    });
});
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js?skin=desert"></script>

</head>


<body>
<div id="wrapper">


<style type="text/css">

#masthead .site-branding {
    margin-bottom: 7px;
}

#masthead .site-branding .site-title {
    font-size: 2.2rem;
    line-height: 1;
    text-transform: uppercase;
    margin: 0;
    margin-bottom: 0.5rem;
}


#masthead .site-description{
	margin:0px;
}




#masthead .site-branding .site-title a {
    display: inline-block;
    position: relative;
    top: -11px;
}

#masthead  a {
    <!-- color: #007bff; -->
    text-decoration: none;
    background-color: transparent;
}



.io-menu-desktop {
    display: block;
    text-align: right;
}
.io-menu-desktop span.io-menu-button-span {
    display: none;
}
.io-menu-desktop ul {
    padding: 0;
    margin: 0;
    list-style: none;
    background: transparent;
    display: block;
}
.io-menu-desktop ul > li {
    margin-right: -4px;
    display: inline-block;
    position: relative;
    height: 30px;
    color: #212529;
    font-size: 12px;
    text-transform: uppercase;
    text-shadow: 0 0 0 rgb(0 0 0 / 0%);
    font-weight: 400;
}
.io-menu-desktop ul > li > a {
    padding: 0;
    line-height: 29px;
    padding-left: 20px;
    padding-right: 20px;
    padding-top: 1px;
    color: #212529;
    font-size: 12px;
    text-transform: uppercase;
    text-shadow: 0 0 0 rgb(0 0 0 / 0%);
    font-weight: 400;
}
.io-menu-desktop a {
    display: block;
    -o-transition: none;
    -moz-transition: none;
    -webkit-transition: none;
    transition: none;
}
.io-menu-desktop > ul > li.current-menu-item > a, .io-menu-desktop > div > ul > li.current-menu-item > a {
    background: rgba(0, 0, 0, 0.01);
}



#colophon {
    margin-top: 70px;
    margin-bottom: 30px;
}
#colophon .site-info {
    text-align: center;
}

</style>


<header id="masthead" class="site-header row">
    <div class="site-branding col-sm-6">
        <span class="site-title" style="font-size: 2.2rem">
            <span>
                <img width="60px" height="60px"
                    src="https://sxm-upload.oss-cn-beijing.aliyuncs.com/imgs/16833443.jpg">
            </span>


            <a href="http://www.sunxvming.com/" rel="home">忧郁的大能猫</a>
        </span>
        <p class="site-description">好奇的探索者，理性的思考者，踏实的行动者。</p>
    </div><!-- .site-branding -->

    <nav id="site-navigation" class="main-navigation col-sm-9">
        <div class="io-menu io-menu-desktop"><span class="io-menu-button io-menu-button-span">≡</span>

            <div class="menu-%e4%b8%bb%e8%8f%9c%e5%8d%95-container">
                <ul id="primary-menu" class="menu">
                    <li id="menu-item-38"
                        class="menu-item menu-item-type-custom menu-item-object-custom current-menu-item current_page_item menu-item-home menu-item-38">
                        <a href="http://www.sunxvming.com">首页</a></li>
                    <li id="menu-item-175"
                        class="menu-item menu-item-type-post_type menu-item-object-page menu-item-175"><a
                            href="http://www.sunxvming.com/">所有文章</a></li>
                    <li id="menu-item-176"
                        class="menu-item menu-item-type-post_type menu-item-object-page menu-item-176"><a
                            href="http://www.sunxvming.com/blog/about-me.html">关于俺</a></li>
                </ul>
            </div>
        </div><!-- .io-menu -->
    </nav><!-- #site-navigation -->
</header>

<div id="header">
<h1 class="title">blog/A-IT/50-应用方向/机器学习/深度学习进阶-自然语言处理</h1>
</div> <!--id="header"-->
 <!--if(title)-->

<p>Table of Contents:</p>
<div id="TOC">
<ul>
<li><a href="#第1章-神经网络的复习">第1章　神经网络的复习</a>
<ul>
<li><a href="#数学和python的复习">1.1　数学和Python的复习</a></li>
</ul></li>
<li><a href="#第2章-自然语言和单词的分布式表示">第2章　自然语言和单词的分布式表示</a>
<ul>
<li><a href="#什么是自然语言处理">2.1　什么是自然语言处理</a></li>
<li><a href="#同义词词典">2.2　同义词词典</a></li>
<li><a href="#基于计数的方法">2.3　基于计数的方法</a></li>
<li><a href="#基于计数的方法的改进">2.4　基于计数的方法的改进</a></li>
<li><a href="#小结">2.5　小结</a></li>
</ul></li>
<li><a href="#第3章-word2vec">第3章　word2vec</a>
<ul>
<li><a href="#基于推理的方法和神经网络">3.1　基于推理的方法和神经网络</a></li>
<li><a href="#简单的word2vec">3.2　简单的word2vec</a></li>
<li><a href="#学习数据的准备">3.3　学习数据的准备</a></li>
<li><a href="#cbow-模型的实现">3.4　CBOW 模型的实现</a></li>
<li><a href="#word2vec的补充说明">3.5　word2vec的补充说明</a></li>
<li><a href="#小结-1">3.6　小结</a></li>
</ul></li>
<li><a href="#第4章-word2vec的高速化">第4章　word2vec的高速化</a>
<ul>
<li><a href="#word2vec的改进①">4.1　word2vec的改进①</a></li>
<li><a href="#word2vec的改进②">4.2　word2vec的改进②</a></li>
<li><a href="#改进版word2vec的学习">4.3　改进版word2vec的学习</a></li>
<li><a href="#wor2vec相关的其他话题">4.4　wor2vec相关的其他话题</a></li>
<li><a href="#小结-2">4.5　小结</a></li>
</ul></li>
<li><a href="#第5章-rnn">第5章　RNN</a>
<ul>
<li><a href="#概率和语言模型">5.1　概率和语言模型</a></li>
<li><a href="#rnn">5.2　RNN</a></li>
<li><a href="#rnn的实现">5.3　RNN的实现</a></li>
<li><a href="#处理时序数据的层的实现本">5.4　处理时序数据的层的实现本</a></li>
<li><a href="#rnnlm的学习和评价">5.5　RNNLM的学习和评价</a></li>
<li><a href="#小结-3">5.6　小结</a></li>
</ul></li>
<li><a href="#第6章-gated-rnn">第6章　Gated RNN</a>
<ul>
<li><a href="#rnn的问题">6.1　RNN的问题</a></li>
<li><a href="#梯度消失和lstm">6.2　梯度消失和LSTM</a></li>
<li><a href="#lstm-的实现">6.3　LSTM 的实现</a></li>
<li><a href="#使用lstm-的语言模型">6.4　使用LSTM 的语言模型</a></li>
<li><a href="#进一步改进rnnlm">6.5　进一步改进RNNLM</a></li>
<li><a href="#小结-4">6.6　小结</a></li>
</ul></li>
<li><a href="#第7章-基于rnn-生成文本">第7章　基于RNN 生成文本</a>
<ul>
<li><a href="#使用语言模型生成文本">7.1　使用语言模型生成文本</a></li>
<li><a href="#seq2seq-模型">7.2　seq2seq 模型</a></li>
<li><a href="#seq2seq-的实现">7.3　seq2seq 的实现</a></li>
<li><a href="#seq2seq-的改进">7.4　seq2seq 的改进</a></li>
<li><a href="#seq2seq的应用">7.5　seq2seq的应用</a></li>
<li><a href="#小结-5">7.6　小结</a></li>
</ul></li>
<li><a href="#第8章-attention">第8章　Attention</a>
<ul>
<li><a href="#attention的结构">8.1　Attention的结构</a></li>
<li><a href="#带attention的seq2seq的实现">8.2　带Attention的seq2seq的实现</a></li>
<li><a href="#attention的评价">8.3　Attention的评价</a></li>
<li><a href="#关于attention的其他话题">8.4　关于Attention的其他话题</a></li>
<li><a href="#attention的应用">8.5　Attention的应用</a></li>
<li><a href="#小结-6">8.6　小结</a></li>
</ul></li>
</ul>
</div>
 <!--if(toc)-->

<p>本书则侧重于循环神经网络和自然语言处理。本书详细介绍了单词向量、LSTM、seq2seq和Attention等自然语言处理中重要的深度学习技术。<br />
简言之，自然语言处理是让计算机理解我们日常所说的语言的技术。</p>
<p>深度学习极大地改善了传统自然语言处理的性能。比如，谷歌的机器翻译性能就基于深度学习获得了显著提升</p>
<h2 id="第1章-神经网络的复习">第1章　神经网络的复习</h2>
<h3 id="数学和python的复习">1.1　数学和Python的复习</h3>
<ul>
<li><p>向量和矩阵<br />
将向量和矩阵扩展到N维的数据集合，就是张量。</p></li>
<li><p>矩阵的对应元素的运算</p></li>
<li><p>广播</p></li>
<li><p>向量内积和矩阵乘积</p></li>
<li><p>矩阵的形状检查</p>
<h3 id="神经网络的推理">1.2　神经网络的推理</h3></li>
<li><p>神经网络的推理的全貌图</p></li>
<li><p>层的类化及正向传播的实现</p>
<h3 id="神经网络的学习">1.3　神经网络的学习</h3></li>
<li><p>损失函数</p></li>
<li><p>导数和梯度</p></li>
<li><p>链式法则</p></li>
<li><p>计算图</p></li>
<li><p>梯度的推导和反向传播的实现</p></li>
<li><p>权重的更新</p>
<h3 id="使用神经网络解决问题">1.4　使用神经网络解决问题</h3></li>
<li><p>螺旋状数据集</p></li>
<li><p>神经网络的实现</p></li>
<li><p>学习用的代码</p></li>
<li><p>Trainer 类</p>
<h3 id="计算的高速化">1.5　计算的高速化</h3></li>
<li><p>位精度</p>
<ul>
<li>NumPy 的浮点数默认使用 64 位的数据类型</li>
<li>使用 32 位浮点数也可以无损地（识别精度几乎不下降）进行神经网络的推理和学习</li>
<li>如果只是神经网络的推理，则即使使用 16 位浮点数进行计算，精度也基本上不会下降，不过，虽然 NumPy 中准备有16 位浮点数，但是普通 CPU 或 GPU 中的运算是用 32 位执行的。处理速度方面并不能获得什么好处</li>
<li>但是，如果是要（在外部文件中）保存学习好的权重，则 16 位浮点数是有用的。具体地说，将权重数据用 16 位精度保存时，只需要 32 位时的一半容量</li>
</ul></li>
<li><p>GPU（CuPy）</p>
<ul>
<li>CuPy 是基于 GPU 进行并行计算的库。要使用 CuPy，需要使用安装有 NVIDIA 的 GPU 的机器，并且需要安装 CUDA 这个面向 GPU 的通用并行计算平台。</li>
<li>CuPy 和 NumPy 拥有共同的 API</li>
</ul></li>
</ul>
<h2 id="第2章-自然语言和单词的分布式表示">第2章　自然语言和单词的分布式表示</h2>
<h3 id="什么是自然语言处理">2.1　什么是自然语言处理</h3>
<p>我们平常使用的语言，如日语或英语，称为自然语言（natural language）。所谓<strong>自然语言处理</strong>（Natural Language Processing，NLP），顾名思义，就是处理自然语言的科学<br />
简单地说，它是一种能够让计算机理解人类语言的技术。换言之，自然语言处理的目标就是让计算机理解人说的话，进而完成对我们有帮助的事情。<br />
问答系统是自然语言处理技术的一个应用</p>
<h3 id="同义词词典">2.2　同义词词典</h3>
<figure>
<img src="https://sxm-upload.oss-cn-beijing.aliyuncs.com/imgs/20231218145734.png" alt="" /><figcaption>image.png</figcaption>
</figure>
<p>像这样，通过对所有单词创建近义词集合，并用图表示各个单词的关系，可以定义单词之间的联系。利用这个“单词网络”，可以教会计算机单词之间的相关性</p>
<p>WordNet<br />
同义词词典的问题</p>
<h3 id="基于计数的方法">2.3　基于计数的方法</h3>
<p>基于Python的语料库的预处理<br />
单词的分布式表示<br />
分布式假设<br />
共现矩阵<br />
向量间的相似度<br />
相似单词的排序</p>
<h3 id="基于计数的方法的改进">2.4　基于计数的方法的改进</h3>
<p>点互信息<br />
降维<br />
基于SVD的降维<br />
PTB 数据集<br />
基于PTB 数据集的评价</p>
<h3 id="小结">2.5　小结</h3>
<h2 id="第3章-word2vec">第3章　word2vec</h2>
<h3 id="基于推理的方法和神经网络">3.1　基于推理的方法和神经网络</h3>
<p>基于计数的方法的问题<br />
基于推理的方法的概要<br />
神经网络中单词的处理方法</p>
<h3 id="简单的word2vec">3.2　简单的word2vec</h3>
<p>CBOW模型的推理<br />
CBOW模型的学习<br />
word2vec的权重和分布式表示</p>
<h3 id="学习数据的准备">3.3　学习数据的准备</h3>
<p>上下文和目标词<br />
转化为one-hot表示</p>
<h3 id="cbow-模型的实现">3.4　CBOW 模型的实现</h3>
<h3 id="word2vec的补充说明">3.5　word2vec的补充说明</h3>
<p>CBOW模型和概率<br />
skip-gram模型<br />
基于计数与基于推理</p>
<h3 id="小结-1">3.6　小结</h3>
<h2 id="第4章-word2vec的高速化">第4章　word2vec的高速化</h2>
<h3 id="word2vec的改进①">4.1　word2vec的改进①</h3>
<p>Embedding层<br />
Embedding层的实现</p>
<h3 id="word2vec的改进②">4.2　word2vec的改进②</h3>
<p>中间层之后的计算问题<br />
从多分类到二分类<br />
sigmoid 函数和交叉熵误差<br />
多分类到二分类的实现<br />
负采样<br />
负采样的采样方法<br />
负采样的实现</p>
<h3 id="改进版word2vec的学习">4.3　改进版word2vec的学习</h3>
<p>CBOW模型的实现<br />
CBOW模型的学习代码<br />
CBOW模型的评价</p>
<h3 id="wor2vec相关的其他话题">4.4　wor2vec相关的其他话题</h3>
<p>word2vec的应用例<br />
单词向量的评价方法</p>
<h3 id="小结-2">4.5　小结</h3>
<h2 id="第5章-rnn">第5章　RNN</h2>
<h3 id="概率和语言模型">5.1　概率和语言模型</h3>
<p>概率视角下的word2vec<br />
语言模型<br />
将CBOW模型用作语言模型？</p>
<h3 id="rnn">5.2　RNN</h3>
<p>循环的神经网络<br />
展开循环<br />
Backpropagation Through Time<br />
Truncated BPTT<br />
Truncated BPTT的mini-batch学习</p>
<h3 id="rnn的实现">5.3　RNN的实现</h3>
<p>RNN 层的实现<br />
Time RNN层的实现</p>
<h3 id="处理时序数据的层的实现本">5.4　处理时序数据的层的实现本</h3>
<p>RNNLM的全貌图<br />
Time 层的实现</p>
<h3 id="rnnlm的学习和评价">5.5　RNNLM的学习和评价</h3>
<p>RNNLM的实现<br />
语言模型的评价<br />
RNNLM的学习代码<br />
RNNLM的Trainer 类</p>
<h3 id="小结-3">5.6　小结</h3>
<h2 id="第6章-gated-rnn">第6章　Gated RNN</h2>
<h3 id="rnn的问题">6.1　RNN的问题</h3>
<p>RNN 的复习<br />
梯度消失和梯度爆炸<br />
梯度消失和梯度爆炸的原因<br />
梯度爆炸的对策</p>
<h3 id="梯度消失和lstm">6.2　梯度消失和LSTM</h3>
<p>LSTM的接口<br />
LSTM层的结构<br />
输出门<br />
遗忘门<br />
新的记忆单元<br />
输入门<br />
LSTM的梯度的流动</p>
<h3 id="lstm-的实现">6.3　LSTM 的实现</h3>
<h3 id="使用lstm-的语言模型">6.4　使用LSTM 的语言模型</h3>
<h3 id="进一步改进rnnlm">6.5　进一步改进RNNLM</h3>
<p>LSTM层的多层化<br />
基于Dropout抑制过拟合<br />
权重共享<br />
更好的RNNLM的实现<br />
前沿研究</p>
<h3 id="小结-4">6.6　小结</h3>
<h2 id="第7章-基于rnn-生成文本">第7章　基于RNN 生成文本</h2>
<h3 id="使用语言模型生成文本">7.1　使用语言模型生成文本</h3>
<p>使用RNN 生成文本的步骤<br />
文本生成的实现<br />
更好的文本生成</p>
<h3 id="seq2seq-模型">7.2　seq2seq 模型</h3>
<p>seq2seq的原理<br />
时序数据转换的简单尝试<br />
可变长度的时序数据<br />
加法数据集</p>
<h3 id="seq2seq-的实现">7.3　seq2seq 的实现</h3>
<p>Encoder类<br />
Decoder类<br />
Seq2seq类<br />
seq2seq的评价</p>
<h3 id="seq2seq-的改进">7.4　seq2seq 的改进</h3>
<p>反转输入数据（Reverse）<br />
偷窥（Peeky）</p>
<h3 id="seq2seq的应用">7.5　seq2seq的应用</h3>
<p>聊天机器人<br />
算法学习<br />
自动图像描述</p>
<h3 id="小结-5">7.6　小结</h3>
<h2 id="第8章-attention">第8章　Attention</h2>
<h3 id="attention的结构">8.1　Attention的结构</h3>
<p>seq2seq存在的问题<br />
编码器的改进<br />
解码器的改进①<br />
解码器的改进②<br />
解码器的改进③</p>
<h3 id="带attention的seq2seq的实现">8.2　带Attention的seq2seq的实现</h3>
<p>编码器的实现<br />
解码器的实现<br />
seq2seq的实现</p>
<h3 id="attention的评价">8.3　Attention的评价</h3>
<p>日期格式转换问题<br />
带Attention的seq2seq的学习<br />
Attention的可视化</p>
<h3 id="关于attention的其他话题">8.4　关于Attention的其他话题</h3>
<p>双向RNN<br />
Attention层的使用方法<br />
seq2seq的深层化和skip connection</p>
<h3 id="attention的应用">8.5　Attention的应用</h3>
<p>GNMT<br />
Transformer<br />
NTM</p>
<h3 id="小结-6">8.6　小结</h3>

<footer id="colophon" >
		<div class="site-info col">
			Powered by <a href="https://github.com/sunxvming/my-blog">my-blog</a>
			<span class="sep"> | </span>
				<span><a target="_blank" href="http://beian.miit.gov.cn">【京ICP备19018538号】</a></span>
			<span><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010502037753">【京公网安备 11010502037753号】</a></span>
		</div><!-- .site-info col -->
        <div class="site-info col"> This page is hosted at <a target="_blank" href="https://github.com/sunxvming">Github</a>.To see the source code you can visit the <a target="_blank" href="https://github.com/sunxvming/my-blog">repo</a> and I'd be glad if you like and star it.</div>
</footer>
</div> <!--wrapper-->
</body>
</html>
